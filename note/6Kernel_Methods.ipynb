{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 核方法\n",
    "对于回归问题和分类问题的线性参数模型，从输入$x$到输出$y$的映射$y(x,w)$形式由可调节参数构成的向量$w$控制。学习阶段，训练数据用于得到参数向量的点估计，或者用来确定这个向量的后验概率分布，然后训练数据被丢弃，新输入的预测纯粹依靠学习得到的参数$w$，该方法也被用于如神经网络等非线性参数模型。\n",
    "对于训练数据点或者其子集在预测阶段仍然保留并被使用的模式识别技术，如最近邻方法。这种方法将每一个新的测试向量分配为训练数据集里距离最近的样本的标签。基于存储的方法将整个训练集的数据存储起来，对未来的数据点进行预测，通常此类方法需要一个度量，来定义整个输入空间中任意两个向量之间的相似度，这种方法训练速度快，但是对于预测数据点的预测速度很慢。\n",
    "许多线性参数模型可被转化为一个等价的“对偶表示”。对偶表示中，预测基础也是在训练数据点处计算核函数的线性组合。对于基于固定非线性的特征空间映射$\\phi(x)$的模型来说，核函数关系如下：\n",
    "$$k(x,x')=\\phi(x)^T\\phi(x')$$\n",
    "\n",
    "所以可以看到，核函数对于其参数是对称的，即$k(x,x')=k(x',x)$，通过考虑特征空间的恒等映射$\\phi(x)=x$，就得到$k(x,x')=x^Tx'$,我们将其称为线性核。\n",
    "用特征空间的内积的方式表示核的概念使得我们能够对许多著名的算法进行有趣的扩展。扩展的方法是使用核技巧（kernel trick），也被称为核替换。一般的思想是如果我们有一个算法，其输入向量$x$只以标量积的形式出现，那么我们可以用一些其他的核来替换标量积。例如，可以使用核替换方法用于主成分分析，进而产生了PCA的非线性变种。核替换的其他例子包括最近邻分类器和核Fisher判别函数。\n",
    "常用的核函数有各种不同的形式，许多核函数只是参数差值的函数，即$k(x,x')=k(x-x')$,这被称为静止核，因为核函数对于输入空间的平移具有不变性。另一种是同质核，也被称为径向基函数，它只依赖于参数之间的距离大小，即$k(x,x')=k(||x-x'||)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 对偶表示\n",
    "许多回归的线性模型和分类的线性模型的公式都可以使用对偶表示重写。使用对偶表示形式，核函数可以自然地产生。\n",
    "我们考虑一个线性模型，其参数通过最小化正则化的平方和误差函数来确定。正则化平方和误差函数：\n",
    "$$J(w)=\\frac{1}{2}\\sum_{n=1}^N \\{w^T \\phi(x_n)-t_n\\}^2+\\frac{\\lambda}{2}w^T w$$\n",
    "\n",
    "其中$\\lambda\\geq 0$,若我们令$J(w)$关于w的导数为0，则可以看到w的解是向量$\\phi(x_n)$的线性组合的形式，系数是w的函数，形式为：\n",
    "$$w=-\\frac{1}{\\lambda}\\sum_{n=1}^N \\{w^T\\phi(x_n)-t_n\\}\\phi(x_n)=\\sum_{n=1}^N a_n \\phi(x_n)=\\Phi^T a$$\n",
    "\n",
    "其中$\\Phi$是设计矩阵，第n行为$\\phi(x_n)^T$。这里向量$a=(a_1,\\dotsb,a_N)^T$,我们定义了：\n",
    "$$a_n=-\\frac{1}{\\lambda}\\{w^T \\phi(x_n)-t_n\\}$$\n",
    "\n",
    "消去w，求解a，可以得到：\n",
    "$$a=(K+\\lambda I_N)^{-1}\\mathbf t$$\n",
    "我们现在不直接对参数向量w进行操作，而是将参数向量a重新计算整理最小平方算法，得到一个对偶表示。如果我们使用$w=\\Phi^T a$代入$J(w)$，可以得到：\n",
    "$$J(a)=\\frac{1}{2}a^T\\Phi \\Phi^T\\Phi \\Phi^T a-a^T\\Phi \\Phi^T\\mathbf t+\\frac{1}{2}\\mathbf t^T\\mathbf t+\\frac{\\lambda}{2}a^T\\Phi \\Phi^T a$$\n",
    "\n",
    "其中$\\mathbf t=(t_1,\\dotsb,t_N)^T$，定义Gram矩阵$K=\\Phi\\Phi^T$,一个$N\\times N$的对称矩阵，元素为：\n",
    "$$K_{nm}=\\phi(x_n)^T\\phi(x_m)=k(x_n,x_m)$$\n",
    "\n",
    "平方和误差函数可写作：\n",
    "$$J(a)=\\frac{1}{2}a^TKK a-a^T K\\mathbf t+\\frac{1}{2}\\mathbf t^T \\mathbf t+\\frac{\\lambda}{2}a^TKa$$\n",
    "将a带入到$y(x)$对于新的输入x，可以得到下面的预测：\n",
    "$$y(x)=w^T \\phi(x)=a^T\\Phi\\phi(x)=k(x)^T (K+\\lambda I_N)^{-1}\\mathbf t$$\n",
    "\n",
    "其中，对于向量$k(x)$,它的元素为$k_n(x)=k(x_n,x)$.\n",
    "因此我们看到对偶公式使得最小平方问题的解完全通过核函数$k(x,x′)$表示。这被称为对偶公式,因为a的解可表示为$\\phi(x)$的线性组合，从而可使用参数向量w恢复出原始的公式。注意，在x处的预测由训练集数据的目标值的线性组合得到。\n",
    "在对偶公式中，我们通过对一个N * N 的矩阵求逆来确定参数向量a，而在原始参数空间公式中，我们要对一个M * M 的矩阵求逆来确定w。由于N 通常远大于M ，因此对偶公式似乎没有实际用处。然而，正如我们将要看到的那样，对偶公式的优点是，它可以完全通过核函数k(x, x′)来表示。于是，我们可以直接针对核函数进行计算，避免了显式地引入特征向量$\\phi(x)$，这使得我们可以隐式地使用高维特征空间，甚至无限维特征空间。\n",
    "基于Gram矩阵的对偶表示的存在是许多线性模型的性质，包括感知器."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5da992886c37b8abe180d0c8366d037510104c1b39a4f76b6590866b65f087ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
