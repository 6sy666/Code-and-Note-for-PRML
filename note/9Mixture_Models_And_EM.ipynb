{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 混合模型与EM算法\n",
    "我们定义观测变量和潜在变量的一个联合概率分布，则对应的观测变量本身的概率分布可以通过求边缘概率分布的方法得到。所以观测变量上的复杂边缘概率分布可以通过观测变量与潜在变量组成的拓展空间上的更加便于计算的联合概率分布表示。\n",
    "混合模型也可以用于数据聚类，而聚类问题可以使用非概率的方法得到。该方法被称为K均值算法。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 K均值算法\n",
    "首先考虑多维空间中的数据点的分组或者聚类问题。对于一个数据集$\\{x_1,\\dotsb,x_N\\}$,它是由D维欧几里德空间中随机变量$x$的N次观测组成的，我们需要将其分为K类，假定K是已经给定的，我们可以直观的认为一组数据点构成的一个聚类中的数据点，聚类内部点的距离应该小于数据点与外部数据点之间的距离。我们引入一组D维向量$\\mu_k$，其中$k=1,2,\\dotsb,K$,并且$\\mu_k$是与第K个聚类相关的一个代表，可以认为$\\mu_k$是聚类的中心，我们想要找到每一个数据点所属的聚类，并且找到一组向量$\\{\\mu_k\\}$,使得每个数据点与相应的$\\mu_k$之间的距离的平方和最小。\n",
    "所以可以使用一组对应的二值指示变量$r_{nk}\\in\\{0,1\\}$，其中k表示的是数据点$x_n$属于K个聚类当中的那一个。如果数据点$x_n$被分配到类别k，则$r_{nk}=1$，对于$j\\neq k，r_{nj}=0$.而目标函数的形式为：$$J=\\sum_{n=1}^N \\sum_{k=1}^K r_{nk}||x_n-\\mu_k||^2$$\n",
    "\n",
    "我们的优化目标是找到$r_{nk}$和$\\mu_k$的值，使得$J$的值能够被最小化。可以使用迭代的方式得到：\n",
    "* 首先，为$\\mu_k$选择一些初始值，然后关于$r_{nk}$最小化$J$，保持$\\mu_k$固定。\n",
    "* 然后关于$\\mu_k$最小化$J$，这个过程中固定$r_{nk}$\n",
    "* 不断重复以上两个过程，直到优化收敛\n",
    "\n",
    "其中更新更新$r_{nk}$和$\\mu_k$的过程分别对应于EM算法当中的E过程和M过程。\n",
    "首先，对于确定$r_{nk}$，由于对于$r_{nk}$来说，$J$是一个线性函数，所以优化过程可以对每一个n进行最优化，只要k的值使得$||x_n-\\mu_k||^2$最小，就令$r_{nk}=1$,所以可以简单地将数据点的聚类设置为最近的聚类中心，可以表示为：\n",
    "$$r_{nk}=\\begin{cases}\n",
    " 1 &若 k=\\argmin_j ||x_n-\\mu_j||^2\\\\\n",
    " 0 &\\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "然后对于$\\mu_k$的优化：因为J关于$\\mu_k$是一个二次函数，可以令J关于$\\mu_k$的导数等于0得到：\n",
    "$$2\\sum_{n=1}^N r_{nk}(x_n-\\mu_k)=0$$\n",
    "\n",
    "可以得到结果为：\n",
    "$$\\mu_k=\\frac{\\sum_n r_{nk}x_n}{\\sum_{n}r_{nk}}$$\n",
    "\n",
    "所以$\\mu_k$的结果等于类别k中所有数据点的均值。\n",
    "\n",
    "由于重新为数据点分配聚类的步骤以及重新计算聚类均值的步骤重复进行，直到聚类的分配不改\n",
    "变（或者直到迭代次数超过了某个最大值）。由于每个阶段都减小了目标函数J的值，因此算\n",
    "法的收敛性得到了保证。然而，算法可能收敛到J的一个局部最小值而不是全局最小值。\n",
    "此时我们使用的是批处理版本，每次更新代表向量时都使用了整个数据集，此外在线随机算法：将Robbins-Monro步骤应用于寻找回归函数根的问题，其中的回归函数是J关于$\\mu_k$的导数，所以可以使用顺序更新的方式：\n",
    "$$\\mu_k^{\\text{new} }=\\mu_k^{\\text{old}}+\\eta_n(x_n-\\mu_k^{\\text{old}})$$\n",
    "\n",
    "K均值算法的基础是将平方欧几里得距离作为数据点与代表向量之间不相似程度的度量。这不仅限制了能够处理的数据变量的类型,而且使得聚类中心的确定对于异常点不具有鲁棒性。我们可以这样推广K均值算法：引入两个向量x和x′之间的一个更加一般的不相似程度的度量$\\mathcal{V}(x,x')$,然后最小化失真度量：\n",
    "$$\\tilde{J}=\\sum_{n=1}^N\\sum_{k=1}^Kr_{nk}\\mathcal{V}(x_n,\\mu_k)$$\n",
    "\n",
    "这就给出了K中心点算法.与之前一样，对于给定的聚类代表$\\mu_k$，E步骤涉及到为每个数据点分配聚类，使得与对应的聚类代表的不相似程度最小。这一步的计算代价为$O(KN )$，与标准的K均值算法的情形相同。对于不相似程度度量的一般选择，M步骤通常比K均值的情形更加复杂，因此通常会将聚类原型限制为等于某个分配到那个聚类的数据向量，因为这使得算法可以适用于任何不相似程度的度量$\\mathcal V(.,.)$，只要它能够被计算。因此，对于每个聚类k，M步骤涉及到在分配到那个聚类的$N_k$个点上的离散搜索，这需要$O(N ^2_k )$次对$\\mathcal V(.,.)$的计算"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.1 图像分割与压缩\n",
    "![](images/2022-12-13-19-11-46.png)\n",
    "![](images/2022-12-13-19-11-59.png)\n",
    "![](images/2022-12-13-19-12-10.png)\n",
    "![](images/2022-12-13-19-12-25.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:30:19) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5da992886c37b8abe180d0c8366d037510104c1b39a4f76b6590866b65f087ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
