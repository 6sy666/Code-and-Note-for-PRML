{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用于回归的线性模型\n",
    "* 回归问题的目标：在给定$D$维输入变量$x$的情况下，预测一个或者多个连续目标变量$t$的值。\n",
    "* 线性回归模型的最简单形式：是输入变量的线性函数。\n",
    "* 基函数：将一组输入变量的非线性函数进行线性组合。这样的模型是参数的线性函数，同时关于输入变量是非线性的。\n",
    "\n",
    "## 3.1 线性基函数模型\n",
    "线性回归：$$y(x,w)=w_0+w_1x_1+\\dotsb w_{D}x_D$$ 关键在于这个函数既是参数的线性函数，也是输入变量的线性函数，所以给模型带来了极大的局限性。所以进行以下拓展：$$y(x,w)=w_0+\\sum_{j=1}^{M-1} w_j\\phi_j(x)$$ 其中$\\phi_j(x)$被称为基函数，参数的总数为$M$.\n",
    "而$w_0$使得数据可以存在任意固定的偏置，这个值通常被称为偏置参数。此时，如果我们定义一个额外的基函数$\\phi_0(x)=1$,则可以得到：$$y(x,w)=\\sum_{j=0}^{M-1}w_j\\phi_j(x)=\\mathbin {w^T\\phi(x)}$$ 其中$w=(w_0,w_1,\\dotsb,w_{M-1})^{T}$,$\\phi=(\\phi_0,\\phi_1,\\dotsb,\\phi_{M-1})^{T}$.\n",
    "在实际应用中，我们首先要进行特征提取，那么提取出的特征可以用$\\phi_j(x)$来表示。\n",
    "对于多项式进行拟合的例子是一个特例，其中每一个基函数都是x的幂指数的形式。但是存在局限性：多项式基函数是输入变量的全局函数，因此对于输入空间一个区域的改变将会影响所有其他区域。所以我们可以将输入空间划分为若干个区域，然后对于每个区域使用不同的多项式函数进行拟合，这样的函数叫做样条函数。\n",
    "其他选择：$$\\phi_j(x)=\\exp{\\big\\{ -\\frac{(x-\\mu_j)^2}{2s^2} \\big\\} }$$ 此外还可以选择sigmod函数$$\\phi_j(x)=\\sigma\\big( \\frac{1}{1+\\exp(-a)} \\big)$$\n",
    "与sigmod函数等价，我们可以使用tanh函数，因为$$tanh(x)=2\\sigma(2x)-1$$ 所以tanh函数的一般线性组合等价于sigmod函数的一般线性组合。\n",
    "此外还可以使用傅利叶基函数。\n",
    "### 3.1.1 最大似然与最小平方\n",
    "假设$t$由确定的函数$y(x,w)$给出，这个函数被附加了高斯噪声：$$t=y(x,w)+\\epsilon$$ 所以$$p(t|x,w,\\beta)=\\mathcal N(t|y(x,w),\\beta^{-1})$$ 所以我们对于假设的平方损失函数，对于x的一个新值，最优预测值由目标变量的条件均值得到：$$\\mathbb {E}[t|x]=\\int tp(t|x){\\rm d}t=y(x,w)$$\n",
    "但是由于我们假设高斯噪声，在给定x的条件下，t的条件分布是单峰的，对于实际任务可能并不合适，因此在之后我们会将其拓展到条件高斯分布的混合。\n",
    "* 似然函数：\n",
    "  对于输入数据集$X=\\{x_1,\\dotsb,x_N\\}$,对应的目标值为$t_1,\\dotsb,t_N$.将目标向量$\\{t_n\\}$组成列向量$\\mathbf t$,假设这些数据点是从分布$$p(t|x,w,\\beta)=\\mathcal N(t|y(x,w),\\beta^{-1})$$ 中独立抽取的，那么我们可以得到以下似然函数的表达式$$p(\\mathbf t|X,w,\\beta)=\\prod_{n=1}^N \\mathcal N(t_n|w^T\\phi(x_n),\\beta^{-1})$$ `注：由于x总会出现在条件变量的位置上，所以为了保持记号的简洁性，在表达式中不再显示写出x`\n",
    "  我们可以得到：$$\\begin{aligned}\n",
    "  \\ln{p(\\mathbf t|w,\\beta)}&=\\sum_{n=1}^N\\ln {\\mathcal N(t_n|w^T\\phi(x_n),\\beta^{-1})}\\\\&=\\frac{N}{2}\\ln{\\beta}-\\frac{N}{2}\\ln{2\\pi}-\\beta E_D(w)\n",
    "  \\end{aligned}$$ 其中$$E_D(w)=\\frac{1}{2}\\sum_{n=1}^N \\{t_n-w^T\\phi(x_n)\\}^2$$ 然后我们就可以使用最大似然的方法求出$w$和$\\beta$.首先对于$w$，我们求导可以得到：$$\\nabla ln{p(\\mathbf t|w,\\beta)}=\\beta \\sum_{n=1}^N \\{t_n-w^T\\phi(x_n)\\}\\phi(x_n)^T$$ 令梯度等于零，得到：$$\\sum_{n=1}^N t_n w^T\\phi(x_n)^T-w^T(\\sum_{n=1}^N\\phi(x_n)\\phi(x_n)^T)$$ 求解$w$，可以得到$$w=( \\Phi^T\\Phi)^{-1}\\Phi^T\\mathbf t$$\n",
    "  我们令$\\Phi=\\begin{bmatrix}\n",
    "   &\\phi_0(x_1) &\\phi_1(x_1) &\\dotsb &\\phi_{M-1}(x_1)\\\\\n",
    "   &\\phi_0(x_2) &\\phi_1(x_2) &\\dotsb &\\phi_{M-1}(x_2)\\\\\n",
    "   &\\dotsb &\\dotsb &\\dotsb &\\dots\\\\\n",
    "   &\\phi_0(x_N) &\\phi_1(x_N) &\\dotsb &\\phi_{M-1}(x_N)\\\\\n",
    "  \\end{bmatrix}$\n",
    "  令$\\Phi^{\\dagger}=(\\Phi^T\\Phi)^{-1}\\Phi^T$称为矩阵$\\Phi$的伪逆矩阵。\n",
    "  **对于$w_0$**:对于$E_D(w)$，如果我们显式的写出偏置参数，那么误差函数可以写为$$E_D(w)=\\frac{1}{2}\\sum_{n=1}^N\\{t_n-w_0-\\sum_{j=1}^{M-1} w_j\\phi_j(x_n)\\}^2$$ 求导令导数等于0，可以得到$$w=\\bar{t}-\\sum_{j=1}^{M-1}w_j\\bar \\phi_j$$ 其中$$\\begin{aligned}\n",
    "    \\bar t&=\\frac{1}{N}\\sum_{n=1}^N t_n\n",
    "   \\\\\\bar{\\phi_j}&=\\frac{1}{N}\\sum_{n=1}^N \\phi_j(x_n)\n",
    "  \\end{aligned}$$ \n",
    "  所以偏置参数$w_0$补偿了目标值的平均值与基函数值的平均值加权求和之间的差。\n",
    "  另外对于$\\beta$进行极大似然估计，可以得到$$\\frac{1} {\\beta_{ML}}=\\sum_{n=1}^N\\{t_n-w^T_{ML}\\Phi(x_n)\\}^2$$ 可以看到噪声精度的倒数等于目标值在回归函数周围的残留方差。\n",
    "\n",
    "### 3.1.2 最小平方的几何描述\n",
    "一个由$t_n$给出坐标轴的N维空间中，每个在N个数据点处估计的基函数$\\phi_j(x_n)$也可以表示为该空间中的一个向量，记为$\\varphi_j$ ，如果基函数的数目M小于N，则M个向量$\\varphi$可以张成一个M维子空间$S$,由于y是$\\varphi_j$的任意组合，所以y在M为子空间的任意位置，所以w的最优解对应的是位于S的与t最小距离的y，所以该解对应于t在子空间上的正交投影。\n",
    "### 3.1.3 顺序学习\n",
    "随机梯度下降：在观测到模式n之后，我们可以立即更新参数：$$w^{\\tau+1}=w^{\\tau}-\\eta \\nabla_w E_n$$\n",
    "对于平方和误差：$$w^{\\tau+1}=w^{\\tau}+\\eta(t_n-w^{(\\tau)T}\\phi_n)\\phi_n$$其中 $\\phi_n=\\phi(x_n)$,这被称为最小均方或者LMS算法。\n",
    "### 3.1.4 正则化最小平方\n",
    "对于平方和误差函数：$$E_D(w)=\\frac{1}{2}\\sum_{n=1}^N \\{t_n-w^T\\phi(x_n)\\}^2$$ 正则化项$$E_w(w)=\\frac{1}{2}w^Tw$$\n",
    "则总误差函数$$\\frac{1}{2}\\sum_{n=1}^N \\{t_n-w^T\\phi(x_n)\\}^2+\\frac{\\lambda}{2}w^Tw$$ 这样的正则化选择被称为权重衰减，因为在顺序学习过程中，它倾向于让权值向0的方向衰减。我们可以得到解析解：$$w=(\\lambda I+\\Phi^T\\Phi)^{-1}\\Phi^T \\mathbf t$$ 这是最小平方解的一个拓展。对于更一般的正则化项：$$\\frac{1}{2}\\sum_{n=1}^N \\{t_n-w^T\\phi(x_n)\\}^2+\\frac{\\lambda}{2}\\sum_{n=1}^N |w_j|^q$$ \n",
    "对于q=1的情况，我们可以得到Lasso,当$\\lambda$足够大的时候，我们可以得到某些系数$w_j$变为0，从而产生一个稀疏模型。Lasso等价于在满足下面限制的条件下最小化平方误差得到的结果：$$\\sum_{j=1}^M|w_j|^q\\leq \\eta$$\n",
    "正则化方法就等同于限制模型复杂度，使之不会出现严重的过拟合。\n",
    "### 3.1.5 多个变量\n",
    "## 3.2 偏差方差分解\n",
    "当我们知道了条件概率分布$p(t|x)$,每一种损失函数都可以给出最优的预测结果。当选择平方损失函数的时候，此时最优预测由条件期望h(x)给出：$$h(x)=\\mathbb E[t|x]=\\int tp(t|x){\\rm d}t$$ 而平方损失函数的期望可以写成：$$\\mathbb E[L]=\\int \\{y(x)-h(x)\\}^2 p(x){\\rm d}x + \\int \\int \\{h(x)-t\\}^2p(x,t){\\rm d}x{\\rm d}t$$\n",
    "这里面的第二项与y无关，可以看作是数据本身的噪声造成的。而对于第一项，我们可以根据对不同的数据集$D$进行建模得到不同的$y$分别计算第一项，然后取均值去评估y的好坏。\n",
    "对于一个特定的数据集$D$,我们可以将被积函数的第一项写为：$$\\{y(x;D)-h(x)\\}^2$$ 可以得到：\n",
    "$$\\begin{aligned}\n",
    "\\{y(x;D)-h(x)\\}^2 &=\\{y(x;D)-\\mathbb E_D[y(x;D)]+\\mathbb E_D[y(x;D)]-h(x)\\}^2\\\\\n",
    "&=\\{y(x;D)-\\mathbb E_D[y(x;D)]\\}^2+\\{\\mathbb E_D[y(x;D)]-h(x)\\}^2+2\\{y(x;D)-\\mathbb E_D[y(x;D)]\\}\\{\\mathbb E_D[y(x;D)]-h(x)\\}\n",
    "\\end{aligned}$$\n",
    "然后我们对这个式子在D上进行求期望操作，可以得到第三项求期望后为0，剩下两项的期望中，第一项可看做偏置的平方，第二项则是方差。\n",
    "因此，由于上面提到平方损失函数的期望可以写成：$$\\mathbb E[L]=\\int \\{y(x)-h(x)\\}^2 p(x){\\rm d}x + \\int \\int \\{h(x)-t\\}^2p(x,t){\\rm d}x{\\rm d}t$$\n",
    "所以平方损失函数的期望可以看作是偏差的平方+方差+噪声：其中\n",
    "$$\\begin{aligned}\n",
    " &偏置^2=\\int \\{\\mathbb E_D[y(x;D)]-h(x)\\}^2p(x){\\rm d}x\\\\\n",
    " &方差=\\int \\mathbb E_D[\\{y(x;D)-\\mathbb E_D[y(x;D)]\\}^2]p(x){\\rm d}x\\\\\n",
    " &噪声=\\int \\int \\{h(x)-t\\}^2p(x,t){\\rm d}x{\\rm d}t\n",
    "\\end{aligned}$$\n",
    "* 灵活的模型：偏置小，方差大\n",
    "* 固定的模型：偏置大，方差小\n",
    "* 所以正则化系数$\\lambda$小的时候方差大，偏置小，$\\lambda$大的时候方差小，偏置大。\n",
    "\n",
    "## 3.3 贝叶斯线性回归\n",
    "\n",
    "### 3.3.1 参数分布"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5da992886c37b8abe180d0c8366d037510104c1b39a4f76b6590866b65f087ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
